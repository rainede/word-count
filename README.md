# word-count
Map Reduce Wordcount

In Map/Reduce we will write three parts of the program:
The Mapper, which implements the Map function,
the Reducer, which implements the Reduce function,
and the main class that configures the complete job.


The input and output of both the Map and the Reduce function is a tuple of key and value. Hadoop defines several special classes to store the values from the tuple. In this first lab we will use the IntWritable object, which stores an int, and the Text object, which stores a String. The provided source code will show how to initialise and use these objects.

Basic flow:
First we run the Mapper: This function splits the full text into all the individual words, and it generates one tuple for each word (with key being a String with the word, and value being 1). The Reducer receives as input the aggregated results from the Mapper execution: for each unique Key, it provides a list of values for that key that have been generated by the Mappers. With that information it must be able to count the number of occurrences for the word, and generate as result for that word another tuple, with key being a Text containing the String with the word, and value being a IntWritable with the total count stored as an integer.

IntWritable and Text are the Hadoop equivalents to int and String Java types. They follow their own hierarchy of classes so that they can be moved over the network during the shuffle and partition steps.
Identify the code for splitting the text into separate words is slightly different, but the
The emit command is equivalent to  write in real MapReduce code.
Look at the signature of TokenizerMapper, and try to understand how it relates to pairs <k1,v1>, <k2,v2>

## Setting up mapreduce
Create an input folder
`$ mkdir input`
WordCount.java a MapReduce program that orchestrates the two classes we have just defined.

In order to create a package ready to be executed by Hadoop go to the project root folder and invoke the ant command

`$ ant clean dist`

If the code compiles correctly a compressed jar file named HadoopTest.jar will appear in a newly created dist/ folder
`ls dist`

Execute the job from the base folder of your project running the following line:

`$ hadoop-local  jar dist/WordCount.jar WordCount input out`

Questions
`$ sort -n -k2 part-00000 | tail`
